# How to integrate models
DarwinKit supports other models by integrating DarwinKit's interface to use DarwinKit's features. Below is a simple example demonstrating how to integrate a simple model.

## Integrating Trainer API

### Defining the Model

First, we need to define a simple model. We can use the `torch.nn.Module` class to define a simple model:

```python
import torch
import torch.nn as nn
from dataclasses import dataclass
from pathlib import Path

@dataclass
class MyModelConfig:
    input_size: int = 12
    hidden_size: int = 12
    output_size: int = 12

class MyModel(nn.Module):
    def __init__(self, config: MyModelConfig):
        super(MyModel, self).__init__()
        self.config = config
        self.fc1 = nn.Linear(config.input_size, config.hidden_size)
        self.fc2 = nn.Linear(config.hidden_size, config.output_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x
```

### Integrating DarwinKit's Trainer API

Next, we need to define a `Trainer` class to train the model. We need to inherit from DarwinKit's `Trainer` and `TrainerConfig` classes to define a new `Trainer` and `TrainerConfig` class:

```python
import random
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm
from darkit.core import Trainer, TrainerConfig

from typing import Optional

print(f"model_cache_path: {model_cache_path}")

@dataclass
class MyTrainerConfig(TrainerConfig):
    device = "cuda"
    lr = 1e-3
    batch_size = 4
    max_step: int = 10000 # Define the maximum training steps
    save_step_interval: int = 10000 # Define the model saving interval

class MyTrainer(Trainer):
    def __init__(self, model: MyModel, config: MyTrainerConfig, **kwargs):
        super().__init__(model, config, **kwargs)
        self.config = config

    def _get_optimizer(self):
        return torch.optim.Adam(self.model.parameters(), lr=self.config.lr)
    
    def train(self, train_dataset, val_dataset=None):
        # Implement custom training logic here
        dataloader = DataLoader(train_dataset, batch_size=self.config.batch_size)
        
        self.optimizer = self._get_optimizer()
        for step, batch in tqdm(enumerate(dataloader)):
            inputs, labels = batch
            inputs = inputs.to(self.config.device)
            labels = labels.to(self.config.device)

            outputs = self.model(inputs)
            loss = F.mse_loss(outputs, labels)
            loss.backward()
            self.optimizer.step()
            
            # Call the save and evaluate methods provided by the Trainer superclass
            # The logic for saving and evaluating is controlled by the parameters provided in the config
            self.current_step = step
            self._auto_save_pretrained()

Trainer.register(MyModel.__name__, MyTrainer)
```

With the above code, we have defined a simple `Trainer` class for training the model. For models integrated with DarwinKit's Trainer API, the following features are provided:

1. Automatic model saving, controlled by the parameters in `TrainerConfig`. Models are uniformly saved to the `DARWIN_KIT_HOME/base/{model.name}` path and named according to the progress at the time of saving, such as `iter-10000-ckpt.pth`.
2. Save model configuration and training configuration to `config.json` and `trainer_config.json` files.
3. Save the tokenizer used by the model to the `tokenizer.json` file.
4. Automatic model evaluation, controlled by the parameters in `TrainerConfig`.

### Integrating Predicter API

To use the model for prediction, you can use the `Predicter` class. The main function of the `Predicter` class is to automatically load the model and tokenizer based on the model name and provide a `predict` method for predicting input data. The `Predicter` class is defined as follows:

```python
from darkit.core import Predicter

class MyPredicter(Predicter):
    def __init__(self, name, model, device="cpu"):
        super().__init__(name, model, device)
    
    @classmethod
    def get_model(cls, name: str, checkpoint: Optional[str] = None):
        # Implement custom logic for getting the model here
        checkpoint_path = cls.get_checkpoint(name, checkpoint)
        config_dict = cls.get_model_config_json(name)
        config = MyModelConfig(**config_dict)
        model = MyModel(config=config)
        checkpoint_dict = torch.load(checkpoint_path, weights_only=True)
        model.load_state_dict(checkpoint_dict["state_dict"], strict=True)
        return model
    
    def predict(self, input):
        input = input.to(self.device)
        output = self.model(input)
        return output
    
MyPredicter.register(MyModel.__name__, MyPredicter)
```
You can refer to the complete code [here](../3.Examples/1.Simple-example.md).

## Advanced Features

### Custom Logging

In addition to the above features, the Trainer API can also implement logging functionality based on other simple configurations to record information during the training process, such as training loss, and it is highly customizable. DarwinKit also provides a web-based visualization tool to view information during the training process.

### Integration Method

You need to define a `LogFieldnames` class to store the log field names. For example, to record the most common training loss, we need to define two fields, `step` and `train_loss`. Each set of `LogFieldnames` data represents the `train_loss` at the current step. In the `Trainer.__init__` method, this class needs to be passed to the superclass's `__init__` method:

```python
@dataclass
class LogFieldnames:
    step: int = 0
    train_loss: float = 0.0

class Trainer(BaseTrainer):
    def __init__(self, model, tokenizer, config, **kwargs):
        super(Trainer, self).__init__(
            model, tokenizer, config, log_fieldnames=LogFieldnames, **kwargs)
        ...
```

To use it, simply call the `self.csv_logger.log` method during training:

```python
class Trainer(BaseTrainer):
    def __init__(self, model, tokenizer, config, **kwargs):
        super(Trainer, self).__init__(
            model, tokenizer, config, log_fieldnames=LogFieldnames, **kwargs)
        ...
    def train(self, train_dataset, val_dataset=None):
        ...
        for step, batch in enumerate(train_dataset):
            ...
            loss = criterion(outputs, labels)
            log_fieldnames = LogFieldnames(
                step=step,
                train_loss=loss,
            )
            self.csv_logger.log(log_fieldnames)
            ...
```

This way, the Trainer will save the recorded `LogFieldnames` information to the `DSPIKE_HOME/model.name/train_log.csv` file during training.

#### Visualizing Train Logs

To view the visualized chart of the recorded logs, you can use the visualization tool provided by DarwinKit. The visualization tool will start a web service in the background, read the data from the `train_log.csv` file, and provide a web page to view the log data.

To display the data we care about in the chart, we need to define the series name and the corresponding x and y axis field names in the Trainer. For example, to display the training loss, we need to specify the series name as `train_loss`, the x-axis as `step`, and the y-axis as `train_loss`:

```python
class Trainer(BaseTrainer):
    _visual_series = {"train_loss": ["step", "train_loss"]}

    def __init__(self, model, tokenizer, config, **kwargs):
        ...
```

There are two ways to start the visualization tool. One is to start it through the CLI:

```bash
DarwinKit start
```

The other is to control whether to start it through the `enable_server` parameter during model training:

```python
trainer = Trainer(model, tokenizer, config, enable_server=True)
...
```

Regardless of the method used to start the visualization page, you can view the log data of previously trained models. You can also view the log data of the currently training model, which will update in real-time.

## Advantages of Integrating DarwinKit

Compared to directly using the model network for training, integrating DarwinKit offers the following advantages:

1. Convenient integration with different datasets and tokenizers.
2. Provides built-in dataset preprocessing methods.
3. Automatic model saving, avoiding model loss due to forgetting to save.
4. Saves model configuration information along with model weights, preventing model unusability due to lost configuration information and facilitating comparison of training effects with different configurations.
5. Automatic model evaluation, eliminating the need to manually set up model evaluation loops.
6. Provides built-in logging tools to record data during training for later review.
7. Provides a web page to view data of both training and completed models, compare parameters and training effects of different models, and view data in real-time during training.
    ![View trained and training models](/static/docs/model_select.png)
    ![Compare model loss curves](/static/docs/loss_chart.png)
