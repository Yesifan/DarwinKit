# How to integrate models
DarwinKit supports other models by integrating DarwinKit's interface to use DarwinKit's features. Below is a simple example demonstrating how to integrate a simple model.

## Integrating Trainer API

### Defining the Model

First, we need to define a simple model. We can use the `torch.nn.Module` class to define a simple model:

```python
import torch
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x
```

Observing the model above, we can extract a `ModelConfig` class to store the model's configuration information:

```python
class SimpleModelConfig:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
```

The corresponding model code is modified as follows:

```python
import torch
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self, config):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(config.input_size, config.hidden_size)
        self.fc2 = nn.Linear(config.hidden_size, config.output_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x
```

### Integrating DarwinKit's Trainer API

Next, we need to define a `Trainer` class to train the model. We need to inherit from DarwinKit's `Trainer` and `TrainerConfig` classes to define a new `Trainer` and `TrainerConfig` class:

```python
from DarwinKit.trainer import Trainer as BaseTrainer, TrainerConfig as BaseTrainerConfig

# As a demonstration, TrainerConfig simply defines a few parameters
# In actual use, more parameters can be defined as needed
class TrainerConfig(BaseTrainerConfig):
    device = "cuda"
    lr = 1e-3
    batch_size = 12
    # These parameters are already defined in BaseTrainerConfig
    # They control some preset training logic of the Trainer API, such as saving checkpoints, evaluating models, etc.
    max_step: int = 80000 * 16 # Define the maximum training steps
    eval_iters: int = 100 # Define evaluation steps
    eval_step_interval: int = 1000 # Define evaluation step interval
    save_step_interval: int = 10000 # Define model saving interval

class Trainer(BaseTrainer):
    def __init__(self, model, tokenizer, config, **kwargs):
        super(Trainer, self).__init__(model, tokenizer, config, **kwargs)
    
    def train(self, train_dataset, val_dataset=None):
        # Implement custom training logic here
        dataloader = DataLoader(train_dataset, batch_size=self.config.batch_size)
        val_dataloader = DataLoader(val_dataset, batch_size=self.config.batch_size)
        
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.lr)

        for step, batch in enumerate(train_dataset):
            inputs, labels = batch
            outputs = self.model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            # Call the save and evaluate methods provided by the Trainer superclass
            # The logic for saving and evaluating is controlled by the parameters provided in the config
            self._auto_validate(val_dataloader)
            self._auto_save_pretrained()
            self.current_step = step
    
    @torch.no_grad()
    def validate(self, val_dataloader) -> torch.Tensor:
        # Implement custom evaluation logic here
        loss = self.model.eval()
        return loss
```

With the above code, we have defined a simple `Trainer` class for training the model. For models integrated with DarwinKit's Trainer API, the following features are provided:

1. Automatic model saving, controlled by the parameters in `TrainerConfig`. Models are uniformly saved to the `DSPIKE_HOME/model.name` path and named according to the progress at the time of saving, such as `iter-10000-ckpt.pth`.
2. Save model configuration and training configuration to `config.json` and `trainer_config.json` files.
3. Save the tokenizer used by the model to the `tokenizer.json` file.
4. Automatic model evaluation, controlled by the parameters in `TrainerConfig`.

### Integrating Predicter API

To use the model for prediction, you can use the `Predicter` class. The main function of the `Predicter` class is to automatically load the model and tokenizer based on the model name and provide a `predict` method for predicting input data. The `Predicter` class is defined as follows:

```python
from DarwinKit.predicter import Predicter as BasePredicter

class Predicter(BasePredicter):
    def __init__(self, model_name, model_path=None, tokenizer_path=None):
        super(Predicter, self).__init__(model_name, model_path, tokenizer_path)

    @classmethod
    def get_model(cls, checkpoint: Optional[str] = None):
        # Implement custom logic for getting the model here
        checkpoint_path = cls.get_checkpoint(name, checkpoint)
        config_dict = cls.get_model_config_json(name)
        config = GPTConfig(**config_dict)
        checkpoint_dict = torch.load(checkpoint_path, weights_only=True)
        model.load_state_dict(checkpoint_dict["model"], strict=True)
        return model
    
    # Optional
    def predict(self, inputs):
        # Implement custom prediction logic here
        outputs = self.model(inputs)
        return outputs
```

## Advanced Features

### Custom Logging

In addition to the above features, the Trainer API can also implement logging functionality based on other simple configurations to record information during the training process, such as training loss, and it is highly customizable. DarwinKit also provides a web-based visualization tool to view information during the training process.

### Integration Method

You need to define a `LogFieldnames` class to store the log field names. For example, to record the most common training loss, we need to define two fields, `step` and `train_loss`. Each set of `LogFieldnames` data represents the `train_loss` at the current step. In the `Trainer.__init__` method, this class needs to be passed to the superclass's `__init__` method:

```python
@dataclass
class LogFieldnames:
    step: int = 0
    train_loss: float = 0.0

class Trainer(BaseTrainer):
    def __init__(self, model, tokenizer, config, **kwargs):
        super(Trainer, self).__init__(
            model, tokenizer, config, log_fieldnames=LogFieldnames, **kwargs)
        ...
```

To use it, simply call the `self.log` method during training:

```python
class Trainer(BaseTrainer):
    def __init__(self, model, tokenizer, config, **kwargs):
        super(Trainer, self).__init__(
            model, tokenizer, config, log_fieldnames=LogFieldnames, **kwargs)
        ...
    def train(self, train_dataset, val_dataset=None):
        ...
        for step, batch in enumerate(train_dataset):
            ...
            loss = criterion(outputs, labels)
            log_fieldnames = LogFieldnames(
                step=step,
                train_loss=loss,
            )
            self.log(log_fieldnames)
            ...
```

This way, the Trainer will save the recorded `LogFieldnames` information to the `DSPIKE_HOME/model.name/train_log.csv` file during training.

#### Visualizing Train Logs

To view the visualized chart of the recorded logs, you can use the visualization tool provided by DarwinKit. The visualization tool will start a web service in the background, read the data from the `train_log.csv` file, and provide a web page to view the log data.

To display the data we care about in the chart, we need to define the series name and the corresponding x and y axis field names in the Trainer. For example, to display the training loss, we need to specify the series name as `train_loss`, the x-axis as `step`, and the y-axis as `train_loss`:

```python
class Trainer(BaseTrainer):
    _visual_series = {"train_loss": ["step", "train_loss"]}

    def __init__(self, model, tokenizer, config, **kwargs):
        ...
```

There are two ways to start the visualization tool. One is to start it through the CLI:

```bash
DarwinKit start
```

The other is to control whether to start it through the `enable_server` parameter during model training:

```python
trainer = Trainer(model, tokenizer, config, enable_server=True)
...
```

Regardless of the method used to start the visualization page, you can view the log data of previously trained models. You can also view the log data of the currently training model, which will update in real-time.

## Advantages of Integrating DarwinKit

Compared to directly using the model network for training, integrating DarwinKit offers the following advantages:

1. Convenient integration with different datasets and tokenizers.
2. Provides built-in dataset preprocessing methods.
3. Automatic model saving, avoiding model loss due to forgetting to save.
4. Saves model configuration information along with model weights, preventing model unusability due to lost configuration information and facilitating comparison of training effects with different configurations.
5. Automatic model evaluation, eliminating the need to manually set up model evaluation loops.
6. Provides built-in logging tools to record data during training for later review.
7. Provides a web page to view data of both training and completed models, compare parameters and training effects of different models, and view data in real-time during training.
    ![View trained and training models](/static/docs/model_select.png)
    ![Compare model loss curves](/static/docs/loss_chart.png)
