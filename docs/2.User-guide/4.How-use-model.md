---
title: How Use Model
authors:
  - "Yesifan@yesifan66@zju.edu.cn"
updated: "2025-01-13"
---
# Training and using models
This tutorial will introduce how to use SpikeGPT to train a text generation model and use the trained model to generate text.

## Training SpikeGPT
SpikeGPT is a model based on spiking neural networks. It can be used to generate text. This tutorial will introduce how to use SpikeGPT to train a text generation model.

### Preparing Data
First, we need to prepare the training data. We will use a small text dataset `Enwik8`, which can be loaded through the `huggingface` API. We can load the dataset with the following code:

```python
from datasets import load_dataset
wikitext = load_dataset("Salesforce/wikitext", "wikitext-103-raw-v1")
```

Since we are training a text generation model, we need a `Tokenizer` to serialize the dataset. We can still use the `huggingface` API to import a `Tokenizer`, such as `GPT2Tokenizer`:

```python
from transformers import GPT2Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
```

> If you encounter difficulties accessing huggingface, you can use the mirror site https://hf-mirror.com/.

### Defining the Model
Next, we need to define a SpikeGPT model. We can use the `SpikeGPT` class to define a SpikeGPT model:

```python
from darkit.models import SpikeGPT, SpikeGPTConfig

model_type = "RWKV"
n_layer = 24
n_embd = 768

config = SpikeGPTConfig(
    tokenizer.vocab_size,
    ctx_len=ctx_len,
    model_type="RWKV",
    n_layer=12,
    n_embd=768,
)
model = SpikeGPT(config).cuda()
```
The `SpikeGPTConfig` class is used to configure the parameters of the SpikeGPT model. Specific parameters can be referenced in the definition of the `SpikeGPTConfig` class.

### Training the Model
Finally, we can use the `Trainer` class provided by this repository to train the model:

```python
from darkit import Trainer
from darkit.models import TrainerConfig

# Parameter configuration
model_name = "GPT-1"
tconf = TrainerConfig(
    name=model_name,
    device=device,
    max_epochs=1,
    epoch_length_fixed=100,
    batch_size=2,
    save_step_interval=1,
)
# Configure the model, dataset, and tokenizer
with Trainer(model, tokenizer=tokenizer, config=tconf) as trainer:
    # Start training
    trainer.train(train_dataset=wikitext_train)
```
The `TrainerConfig` class is used to configure the training parameters. Specific parameters can be referenced in the definition of the `TrainerConfig` class.

![SpikeGPT Training](/static/docs/SpikeGPTTrain.png)

### Saving and Loading the Model
During model training, the logic for saving the model is generally controlled according to the settings in `TrainerConfig`. For example, in the `TrainerConfig` of `SpikeGPT`, we can set `save_step_interval` to control the interval for saving the model.

The path for saving the model is determined based on the values of `tconf.name` and the `DARWIN_KIT_HOME` environment variable.

### Generating Text
After training is complete, the trained model can be loaded using the model name set during training. We can use the following code to generate text:

```python
from darkit.lm.main import Predicter
predicter = Predicter.from_pretrained(model_name)

prompt = "I am"

print(prompt, end="")
for char in predicter.predict(prompt, ctx_len=ctx_len):
    print(char, end="", flush=True)
print()
```

We can use the `predict` method to generate text. The `predict` method accepts a `prompt` and a `ctx_len` parameter. `prompt` is a string used to initialize the state of the generator. `ctx_len` is an integer used to control the length of the generated text.

The schematic diagram is as follows:

![SpikeGPT Run](/static/docs/SpikeGPTRun.gif)

## Complete Code
```python
from datasets import load_dataset
from transformers import AutoTokenizer, GPT2Tokenizer
from darkit.lm.main import Trainer, Predicter
from darkit.lm.models.SpikeGPT import SpikeGPT, SpikeGPTConfig, TrainerConfig

device = "cuda"
ctx_len = 64

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

wikitext = load_dataset("Salesforce/wikitext", "wikitext-103-raw-v1")
wikitext_train = wikitext["train"]  # type: ignore

model_name = "GPT-1"
config = SpikeGPTConfig(
    tokenizer.vocab_size,
    ctx_len=ctx_len,
    model_type="RWKV",
    n_layer=12,
    n_embd=768,
)
model = SpikeGPT(config)
tconf = TrainerConfig(
    name=model_name,
    device=device,
    max_epochs=1,
    epoch_length_fixed=100,
    batch_size=2,
    save_step_interval=1,
)
with Trainer(model, tokenizer=tokenizer, config=tconf) as trainer:
    trainer.train(train_dataset=wikitext_train)

# Test the model
predicter = Predicter.from_pretrained(model_name)
prompt = "hello world"
print(prompt, end="")
for char in predicter.predict(prompt, ctx_len=ctx_len):
    print(char, end="", flush=True)
print()
```
